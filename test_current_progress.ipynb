{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.datasets.mimic import make_mimic\n",
    "from src.models.vision_transformer import PatchEmbed\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda:0\"\n",
    "    torch.cuda.set_device(device=device)\n",
    "else:\n",
    "    device=\"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.masks.multiblock import MaskCollator\n",
    "import numpy as np\n",
    "\n",
    "dataset, data_loader = make_mimic(None, 1, collator=MaskCollator(input_size=(12,5000), patch_size=(1,100)))\n",
    "iterator = enumerate(data_loader)\n",
    "idx, (data,masks_enc,masks_pred) = next(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77]) torch.Size([1, 143])\n",
      "[tensor([[ 31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  81,\n",
      "          82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93, 131, 132,\n",
      "         133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 181, 182, 183,\n",
      "         184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 231, 232, 233, 234,\n",
      "         235, 236, 237, 238, 239, 240, 241, 242, 243, 281, 282, 283, 284, 285,\n",
      "         286, 287, 288, 289, 290, 291, 292, 293, 331, 332, 333, 334, 335, 336,\n",
      "         337, 338, 339, 340, 341, 342, 343, 381, 382, 383, 384, 385, 386, 387,\n",
      "         388, 389, 390, 391, 392, 393, 431, 432, 433, 434, 435, 436, 437, 438,\n",
      "         439, 440, 441, 442, 443, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "         490, 491, 492, 493, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540,\n",
      "         541, 542, 543]], device='cuda:0'), tensor([[ 11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  61,\n",
      "          62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73, 111, 112,\n",
      "         113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 161, 162, 163,\n",
      "         164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 211, 212, 213, 214,\n",
      "         215, 216, 217, 218, 219, 220, 221, 222, 223, 261, 262, 263, 264, 265,\n",
      "         266, 267, 268, 269, 270, 271, 272, 273, 311, 312, 313, 314, 315, 316,\n",
      "         317, 318, 319, 320, 321, 322, 323, 361, 362, 363, 364, 365, 366, 367,\n",
      "         368, 369, 370, 371, 372, 373, 411, 412, 413, 414, 415, 416, 417, 418,\n",
      "         419, 420, 421, 422, 423, 461, 462, 463, 464, 465, 466, 467, 468, 469,\n",
      "         470, 471, 472, 473, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520,\n",
      "         521, 522, 523]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "data = data.to(device)\n",
    "masks_enc = [u.to(device) for u in masks_enc]\n",
    "masks_pred = [u.to(device) for u in masks_pred]\n",
    "print(masks_enc[0].shape, masks_pred[0].shape)\n",
    "print(masks_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 600, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder =  PatchEmbed(sig_size=[12,5000], patch_size=[1,100], in_chans=1, embed_dim=768).to(device)\n",
    "embedder(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "torch.Size([1, 600, 192])\n",
      "torch.Size([1, 600, 192])\n"
     ]
    }
   ],
   "source": [
    "from src.models.vision_transformer import vit_tiny, vit_predictor\n",
    "vit = vit_tiny(sig_size=[12,5000], patch_size=[1,100], in_chans=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 600, 192]) tensor([[[-1.0816, -0.9743, -1.2279,  ...,  1.0870,  1.2226,  1.2479],\n",
      "         [ 0.6709,  0.3780, -0.0387,  ...,  1.0209,  1.2078,  1.2800],\n",
      "         [ 0.9280,  0.8592,  0.6504,  ...,  0.9852,  1.1914,  1.2185],\n",
      "         ...,\n",
      "         [-0.2804,  0.8453,  0.1495,  ...,  1.1970,  1.0874,  1.3430],\n",
      "         [-1.8311,  0.9788,  0.7349,  ...,  1.1908,  1.1223,  1.3676],\n",
      "         [-2.1792,  0.1822,  0.6932,  ...,  1.1661,  1.2745,  1.2774]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = vit.forward(data)\n",
    "print(out.shape, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = vit_predictor(num_patches=600,embed_dim=192).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 600, 192])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor_pos_embed: torch.Size([1, 600, 384])\n",
      "x_pos_embed torch.Size([1, 600, 384])\n",
      "x: torch.Size([1, 600, 384])\n",
      "m: torch.Size([1, 77])\n",
      "mask_keep: torch.Size([1, 77, 384])\n",
      "torch.Size([1, 77, 384])\n",
      "temp: torch.Size([1, 77, 384])\n",
      "x: torch.Size([1, 600, 384])\n",
      "m: torch.Size([1, 77])\n",
      "mask_keep: torch.Size([1, 77, 384])\n",
      "torch.Size([1, 77, 384])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (600) must match the size of tensor b (77) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Arno\\LRZ Sync+Share\\Master Semester 3\\IDP\\IDP at AIM\\ijepa\\test_current_progress.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Arno/LRZ%20Sync%2BShare/Master%20Semester%203/IDP/IDP%20at%20AIM/ijepa/test_current_progress.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m out2 \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39;49mforward(out, masks_enc, masks_pred)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Arno/LRZ%20Sync%2BShare/Master%20Semester%203/IDP/IDP%20at%20AIM/ijepa/test_current_progress.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(out2\u001b[39m.\u001b[39mshape, out2)\n",
      "File \u001b[1;32mc:\\Users\\Arno\\LRZ Sync+Share\\Master Semester 3\\IDP\\IDP at AIM\\ijepa\\src\\models\\vision_transformer.py:343\u001b[0m, in \u001b[0;36mVisionTransformerPredictor.forward\u001b[1;34m(self, x, masks_x, masks)\u001b[0m\n\u001b[0;32m    341\u001b[0m temp \u001b[39m=\u001b[39m apply_masks(x_pos_embed, masks_x)\n\u001b[0;32m    342\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtemp:\u001b[39m\u001b[39m\"\u001b[39m, temp\u001b[39m.\u001b[39mshape)\n\u001b[1;32m--> 343\u001b[0m x \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m apply_masks(x_pos_embed, masks_x)\n\u001b[0;32m    345\u001b[0m _, N_ctxt, D \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[0;32m    347\u001b[0m \u001b[39m# -- concat mask tokens to x\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (600) must match the size of tensor b (77) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "out2 = pred.forward(out, masks_enc, masks_pred)\n",
    "print(out2.shape, out2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
